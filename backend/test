{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Практическое задание 3. Градиентный спуск своими руками\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 05.10.2020\n",
    "\n",
    "Мягкий дедлайн: 01:59MSK 19.10.2020 (за каждый день просрочки снимается 1 балл)\n",
    "\n",
    "Жесткий дедлайн: 01:59MSK 22.10.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов + 2 балла бонус.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "Также оценка может быть снижена за плохо читаемый код и плохо считываемые диаграммы.\n",
    "\n",
    "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. Сам ноутбук называйте в формате homework-practice-03-gd-Username.ipynb, где Username — ваша фамилия.\n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "**Оценка**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по одному объекту, выбранному случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь для каждой итерации, начиная с нулевой (до первого шага).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.01207486 0.01859699 0.0281264  0.02201357 0.0215206  0.02592438\n",
      " 0.02064316 0.02108876 0.02425907 0.0159105  0.01564665 0.01442205\n",
      " 0.02034785 0.02165408 0.01077841 0.03164574 0.01568204 0.02919318\n",
      " 0.02221825 0.02554652 0.0193532  0.01913181 0.02523096 0.01680687\n",
      " 0.02192622 0.00914029 0.02772306 0.01701024 0.02070649 0.02223879\n",
      " 0.02197421 0.02763469 0.02547046 0.02340721 0.02309273 0.02328853\n",
      " 0.01790383 0.01682929 0.0186696  0.02268519 0.01860318 0.00846609\n",
      " 0.01949356 0.02425931 0.01529869 0.01795738 0.01862045 0.02217756\n",
      " 0.02359    0.01765456 0.01976754 0.02554384 0.01772268 0.02127965\n",
      " 0.01455714 0.02397249 0.03149372 0.03151974 0.02229766 0.01886175\n",
      " 0.02131208 0.02016649 0.01534843 0.01991414 0.02760405 0.02148196\n",
      " 0.01148055 0.01799745 0.01156671 0.03244232 0.01229452 0.01695001\n",
      " 0.00835836 0.01740852 0.00996803 0.02405149 0.0267495  0.02106294\n",
      " 0.01966007 0.01693521 0.01502357 0.01813863 0.0233761  0.02410192\n",
      " 0.01920999 0.02200072 0.01508275 0.02728832 0.01911384 0.02187794\n",
      " 0.01666055 0.01930934 0.01968639 0.02417625 0.02470129 0.01789982\n",
      " 0.01533034 0.0207585  0.02607808 0.01970836] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.02296365 0.03385593 0.05110466 0.04212496 0.03538241 0.04470336\n",
      " 0.03920192 0.03590732 0.04102202 0.02719733 0.02883509 0.02378165\n",
      " 0.03583596 0.03584525 0.01941179 0.05605371 0.0266517  0.05257213\n",
      " 0.03709117 0.04602194 0.03543319 0.03354734 0.04355141 0.02904992\n",
      " 0.04007938 0.01576306 0.04894927 0.02871735 0.03815056 0.03889153\n",
      " 0.03881648 0.04943725 0.04294241 0.03928442 0.04214322 0.04112214\n",
      " 0.03325301 0.02965391 0.03521906 0.03956438 0.0341713  0.01525383\n",
      " 0.03491823 0.04463857 0.02765613 0.0327499  0.03173273 0.03724591\n",
      " 0.04126352 0.03206488 0.03347186 0.04567542 0.03296741 0.03885127\n",
      " 0.02394252 0.04189945 0.05465464 0.0553163  0.03910626 0.03210963\n",
      " 0.04132253 0.03483353 0.02732306 0.03364992 0.04984655 0.03818457\n",
      " 0.02025609 0.03281967 0.01847884 0.05629907 0.021475   0.0312466\n",
      " 0.01507362 0.03225835 0.01771661 0.04115303 0.04737548 0.03873866\n",
      " 0.03597986 0.02802594 0.02691746 0.02974651 0.04114435 0.04452056\n",
      " 0.03328482 0.0411728  0.02646643 0.04841028 0.03388677 0.03867518\n",
      " 0.02870025 0.03243305 0.03183723 0.0405701  0.04397273 0.02834461\n",
      " 0.02751418 0.03643735 0.04528453 0.03218205] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.02814188 0.04180587 0.06410439 0.05216877 0.04502587 0.05677936\n",
      " 0.04830128 0.04551265 0.05163798 0.03383374 0.03584247 0.03074733\n",
      " 0.04524475 0.04561477 0.02424937 0.06975516 0.03310641 0.064938\n",
      " 0.04718649 0.05745127 0.04321856 0.04131332 0.05426001 0.0378368\n",
      " 0.0494012  0.01970694 0.06101708 0.03676028 0.04636769 0.04866333\n",
      " 0.04960796 0.06172186 0.05477778 0.04986528 0.0521758  0.05062719\n",
      " 0.04082049 0.03755922 0.04331413 0.04930368 0.04208356 0.01986476\n",
      " 0.04255623 0.0551947  0.03330603 0.04168046 0.03916897 0.0471891\n",
      " 0.0521818  0.03987676 0.04311844 0.05622368 0.04135229 0.04841184\n",
      " 0.02990695 0.05269154 0.06879883 0.06896072 0.04819283 0.04053838\n",
      " 0.05049192 0.04392302 0.03498207 0.04251744 0.06118062 0.04733831\n",
      " 0.02521976 0.04045187 0.02411134 0.07079152 0.02649117 0.03950638\n",
      " 0.01867598 0.03990067 0.02252654 0.05106798 0.05818711 0.04733093\n",
      " 0.04498922 0.03599407 0.03426473 0.03811286 0.05188487 0.0558995\n",
      " 0.04125881 0.05051706 0.03290917 0.06045151 0.04254285 0.04759317\n",
      " 0.03597276 0.04159308 0.04086325 0.05198676 0.05404245 0.0365858\n",
      " 0.03391962 0.044687   0.05734301 0.04080829] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.03276054 0.04922258 0.07536847 0.06109998 0.05479729 0.067821\n",
      " 0.05662943 0.05500887 0.06290541 0.04049683 0.04217568 0.03734099\n",
      " 0.05366571 0.05545819 0.02912198 0.08351245 0.03974136 0.07703635\n",
      " 0.05703969 0.06854401 0.05124281 0.05009033 0.06529783 0.04537861\n",
      " 0.05892308 0.02382856 0.07304279 0.04401916 0.05508157 0.05841392\n",
      " 0.05897155 0.07312036 0.06640325 0.06031888 0.06127508 0.0606795\n",
      " 0.04798572 0.04471579 0.05104134 0.05900706 0.04988492 0.02347909\n",
      " 0.05073734 0.06550553 0.03975952 0.04933946 0.04745294 0.05746235\n",
      " 0.06245777 0.04712283 0.05204504 0.06728966 0.0484573  0.05730051\n",
      " 0.03624709 0.06322346 0.0827796  0.0823722  0.05802031 0.04954093\n",
      " 0.05871763 0.05270457 0.04140045 0.05123873 0.07238246 0.05646797\n",
      " 0.02988807 0.04812817 0.02977115 0.08481487 0.031893   0.04630693\n",
      " 0.02226044 0.04707436 0.02691192 0.06190035 0.06977575 0.05580011\n",
      " 0.05293287 0.04420755 0.04026003 0.04619913 0.06169737 0.06557248\n",
      " 0.04987286 0.05931162 0.03924193 0.07184511 0.05054028 0.05694805\n",
      " 0.04370368 0.05048865 0.05043992 0.06309232 0.06478717 0.04522749\n",
      " 0.0408018  0.05386049 0.0688082  0.05010641] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.03323323 0.04989331 0.07642388 0.06199096 0.05544613 0.0687068\n",
      " 0.05744108 0.05569608 0.06366035 0.04099698 0.04276553 0.03780126\n",
      " 0.05438671 0.05612269 0.02950544 0.08459163 0.04022421 0.07805681\n",
      " 0.05773987 0.06945881 0.05191998 0.05070004 0.06610859 0.04599463\n",
      " 0.05970574 0.02412285 0.07398612 0.04458482 0.05580772 0.05916153\n",
      " 0.05977994 0.07410305 0.06722747 0.06105913 0.06212058 0.06144313\n",
      " 0.04864977 0.04531544 0.05175741 0.05975741 0.05055771 0.02382167\n",
      " 0.05138215 0.06639656 0.04025728 0.0500403  0.04801325 0.05815524\n",
      " 0.06327688 0.04776683 0.05271276 0.068152   0.04915722 0.05808774\n",
      " 0.03666435 0.06404035 0.08383449 0.08343399 0.05873933 0.05014028\n",
      " 0.05957949 0.05337828 0.04198025 0.05187337 0.07333941 0.05720067\n",
      " 0.03028115 0.04876822 0.03011683 0.08590365 0.03228678 0.0469783\n",
      " 0.0225544  0.04772688 0.02727578 0.06264192 0.07065283 0.05655245\n",
      " 0.05367507 0.04473484 0.0408305  0.04675739 0.06251952 0.06651291\n",
      " 0.05048248 0.06013949 0.03974717 0.07279552 0.0512153  0.0576676\n",
      " 0.04423964 0.05111685 0.05101599 0.06387557 0.06560776 0.04573358\n",
      " 0.04132277 0.05452039 0.06970083 0.05067922] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.03470089 0.05210696 0.08025269 0.06530619 0.0582259  0.07245986\n",
      " 0.06045518 0.05887198 0.06703578 0.04246711 0.04495032 0.04008294\n",
      " 0.0571866  0.05911562 0.03114618 0.0888514  0.04170044 0.08142611\n",
      " 0.06096709 0.07318988 0.05401815 0.05313996 0.06935432 0.04937139\n",
      " 0.06282843 0.02537243 0.07767783 0.04706508 0.0582658  0.06246758\n",
      " 0.06347353 0.07760542 0.07128169 0.06424172 0.06490961 0.06408349\n",
      " 0.05089506 0.04779366 0.05425162 0.06238671 0.05272402 0.02553245\n",
      " 0.05333918 0.07002736 0.04153352 0.05326735 0.05006654 0.06132306\n",
      " 0.06678897 0.05029479 0.05615942 0.07126756 0.05174824 0.06123377\n",
      " 0.03807954 0.06779446 0.08841362 0.0875099  0.06152793 0.05318807\n",
      " 0.06240755 0.05614062 0.04473601 0.05465866 0.07635711 0.05969887\n",
      " 0.03156933 0.0509759  0.03217042 0.09040592 0.03374398 0.04971321\n",
      " 0.02361705 0.0499591  0.02884246 0.06550209 0.07386883 0.0587468\n",
      " 0.05647312 0.04764665 0.04322322 0.04932808 0.06575262 0.07014779\n",
      " 0.0528365  0.06267052 0.04160696 0.07626932 0.05378105 0.06006513\n",
      " 0.0466806  0.05433196 0.05413828 0.06783186 0.06857944 0.04856544\n",
      " 0.04340438 0.05689562 0.07374736 0.05353455] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.0361995  0.05418878 0.08335415 0.06867751 0.06060506 0.07508751\n",
      " 0.06340273 0.06144431 0.07042874 0.04409529 0.04688774 0.04165071\n",
      " 0.05929508 0.06154759 0.03294714 0.09311291 0.04330305 0.0851281\n",
      " 0.06317877 0.0768777  0.0567362  0.05620406 0.07237364 0.05135027\n",
      " 0.06621027 0.02661192 0.08133454 0.0485432  0.06133862 0.06522071\n",
      " 0.06596378 0.08082682 0.07435203 0.06687429 0.06738982 0.06720656\n",
      " 0.05315987 0.04976102 0.05718875 0.06513518 0.05551596 0.02657157\n",
      " 0.05591327 0.07353112 0.04378674 0.0556922  0.05240102 0.06427004\n",
      " 0.06971423 0.05236296 0.0583981  0.07491957 0.054004   0.06401806\n",
      " 0.03944001 0.07073108 0.09247955 0.09118479 0.06455469 0.05608795\n",
      " 0.06540176 0.05851571 0.04632582 0.05668748 0.07959572 0.06252507\n",
      " 0.03274289 0.05369376 0.03359005 0.09414121 0.03539125 0.05166117\n",
      " 0.02482232 0.05245352 0.03026165 0.06873316 0.0774683  0.06146936\n",
      " 0.05878306 0.05005735 0.04457103 0.05100832 0.06834313 0.07302694\n",
      " 0.05552509 0.06570528 0.04327184 0.07944142 0.05598966 0.06293117\n",
      " 0.04917283 0.05668472 0.05673828 0.0707401  0.07208286 0.05057021\n",
      " 0.04589598 0.05983822 0.07689716 0.05592308] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.04172925 0.06234631 0.09642672 0.07882918 0.06947493 0.08631499\n",
      " 0.07247046 0.06998367 0.08000557 0.05104962 0.05384411 0.04772744\n",
      " 0.06841909 0.07032532 0.03749075 0.10619516 0.04993978 0.09784391\n",
      " 0.07208322 0.08776849 0.06490907 0.06349829 0.08238124 0.0586743\n",
      " 0.07508523 0.03023646 0.0929138  0.05588868 0.06964198 0.07398421\n",
      " 0.07578986 0.09315601 0.08457611 0.07660953 0.07771889 0.0766856\n",
      " 0.06088744 0.05727397 0.0655304  0.07487611 0.06385823 0.03069162\n",
      " 0.06386771 0.08348877 0.04999609 0.06386262 0.05951708 0.07318617\n",
      " 0.07987633 0.05983234 0.06659255 0.08532469 0.06248419 0.07317774\n",
      " 0.0452763  0.08026909 0.10545827 0.10427953 0.07312956 0.06321488\n",
      " 0.0750638  0.06708099 0.05312827 0.06465978 0.09132226 0.0718302\n",
      " 0.03782293 0.06149357 0.0380546  0.10767287 0.04023416 0.05964108\n",
      " 0.02842752 0.06035762 0.03482449 0.07814705 0.0879298  0.07070627\n",
      " 0.06768401 0.05658582 0.05149773 0.05843187 0.07870688 0.08424185\n",
      " 0.06313319 0.07574789 0.04953421 0.09135107 0.06446125 0.0719531\n",
      " 0.05567224 0.06447754 0.06417107 0.08051208 0.08195396 0.05720044\n",
      " 0.05198245 0.06787007 0.08775815 0.06319899] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n",
      "[0.0443669  0.06624224 0.103002   0.08433617 0.07470613 0.09184091\n",
      " 0.07687235 0.07454515 0.08590001 0.05507335 0.0572331  0.05112103\n",
      " 0.07299155 0.07525524 0.04038539 0.11337001 0.05367034 0.10484905\n",
      " 0.0763334  0.09391644 0.06946667 0.06804168 0.08749753 0.0622576\n",
      " 0.07988951 0.03229487 0.09931678 0.05934021 0.07399974 0.07812688\n",
      " 0.08065088 0.09969038 0.0899808  0.08206362 0.08258206 0.08205587\n",
      " 0.06455801 0.06128779 0.07035559 0.08047389 0.06904312 0.03285745\n",
      " 0.0682388  0.08838217 0.05378602 0.06810718 0.06346177 0.0786013\n",
      " 0.08540393 0.06313794 0.07067758 0.09123885 0.06701756 0.07770471\n",
      " 0.04842728 0.08471727 0.11257246 0.11088695 0.07765866 0.06750121\n",
      " 0.07970049 0.07175297 0.05602113 0.06841413 0.09715799 0.0772886\n",
      " 0.04041966 0.06622893 0.0406477  0.11458429 0.04292448 0.06345652\n",
      " 0.03056478 0.06491999 0.03764773 0.08370829 0.09340755 0.0756996\n",
      " 0.07203    0.060545   0.05442055 0.06216008 0.08408189 0.08976269\n",
      " 0.06765396 0.08136515 0.05258595 0.09760953 0.06893493 0.07706363\n",
      " 0.05959466 0.06865112 0.06869033 0.08574719 0.08748274 0.06081909\n",
      " 0.05566281 0.07245241 0.09330354 0.06716251] [0.21332605 0.71187228 0.15060163 0.78524793 0.30937106 0.55362724\n",
      " 0.96615626 0.8562908  0.40386172 0.759524   0.15966996 0.87669923\n",
      " 0.23409498 0.10964574 0.58716701 0.70942276 0.65517815 0.8952804\n",
      " 0.76834679 0.85347767 0.2682393  0.49063206 0.95998247 0.11849685\n",
      " 0.54930399 0.37745961 0.90726252 0.81165738 0.58885775 0.41862917\n",
      " 0.59356363 0.48415341 0.33251626 0.2472786  0.94368973 0.6762882\n",
      " 0.92661822 0.33101674 0.10166126 0.00586272 0.14899679 0.23085985\n",
      " 0.51079227 0.88942764 0.64915585 0.1303195  0.92617332 0.78194934\n",
      " 0.90199405 0.88952411 0.43823012 0.7068876  0.94613841 0.53698194\n",
      " 0.47699791 0.26001223 0.5185983  0.32189306 0.20771376 0.31566309\n",
      " 0.31299354 0.4779894  0.4805029  0.30466172 0.40713511 0.89774956\n",
      " 0.89937573 0.94470893 0.40305245 0.84171945 0.61935869 0.43089867\n",
      " 0.01943894 0.81918988 0.88828412 0.18400642 0.00667444 0.3723046\n",
      " 0.64795823 0.9674364  0.02155949 0.64210446 0.91294412 0.79876078\n",
      " 0.34585703 0.93703187 0.55069556 0.60744976 0.08682096 0.29995683\n",
      " 0.11152397 0.50294031 0.51393826 0.40892825 0.96503864 0.64128003\n",
      " 0.46394852 0.25275201 0.52723283 0.93108068]\n"
     ]
    }
   ],
   "source": [
    "# LinearRegression\n",
    "\n",
    "regression = LinearRegression(\n",
    "    descent = StochasticDescent(lambda_ = lambda_, w0 = w0, batch_size = 2),\n",
    "    tolerance = tolerance,\n",
    "    max_iter = max_iter\n",
    ")\n",
    "\n",
    "regression.fit(X, y)\n",
    "\n",
    "assert len(regression.loss_history) == max_iter, 'Loss history failed'\n",
    "\n",
    "prediction = regression.predict(X)\n",
    "\n",
    "assert prediction.shape[0] == num_objects, 'Predict failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ваше решение прошло все тесты локально, то теперь осталось протестировать его в Яндекс Контесте - **https://contest.yandex.ru/contest/19551**.\n",
    "\n",
    "Для каждой задачи из контеста вставьте ссылку на успешную посылку:\n",
    "\n",
    "* **GradientDescent**: https://contest.yandex.ru/contest/19551/run-report/36643092/\n",
    "* **StochasticDescent**: https://contest.yandex.ru/contest/19551/run-report/36643432/\n",
    "* **MomentumDescent**: https://contest.yandex.ru/contest/19551/run-report/36643571/\n",
    "* **Adagrad**: https://contest.yandex.ru/contest/19551/run-report/36643785/\n",
    "* **LinearRegression**: https://contest.yandex.ru/contest/19551/run-report/36662313/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать датасет объявлений по продаже машин на немецком Ebay. В задаче предсказания целевой переменной для нас будет являться цена.\n",
    "Для дальнейшей работы сделайте следующее:\n",
    "* Проведите разумную предобработку данных.\n",
    "* Замените целевую переменную на её логарифм.\n",
    "* Разделите данные на обучающую, валидационную и тестовую выборки в отношении 3:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('autos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>dateCreated</th>\n",
       "      <th>lastSeen</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>golf</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>75</td>\n",
       "      <td>150000</td>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>2016-03-17 17:40:17</td>\n",
       "      <td>91074</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skoda</td>\n",
       "      <td>fabia</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>manuell</td>\n",
       "      <td>diesel</td>\n",
       "      <td>nein</td>\n",
       "      <td>69</td>\n",
       "      <td>90000</td>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-03-31 00:00:00</td>\n",
       "      <td>2016-04-06 10:17:21</td>\n",
       "      <td>60437</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmw</td>\n",
       "      <td>3er</td>\n",
       "      <td>limousine</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ja</td>\n",
       "      <td>102</td>\n",
       "      <td>150000</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-04-04 00:00:00</td>\n",
       "      <td>2016-04-06 19:17:07</td>\n",
       "      <td>33775</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peugeot</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>cabrio</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>109</td>\n",
       "      <td>150000</td>\n",
       "      <td>2004</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-04-01 00:00:00</td>\n",
       "      <td>2016-04-05 18:18:39</td>\n",
       "      <td>67112</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mazda</td>\n",
       "      <td>3_reihe</td>\n",
       "      <td>limousine</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>105</td>\n",
       "      <td>150000</td>\n",
       "      <td>2004</td>\n",
       "      <td>12</td>\n",
       "      <td>2016-03-26 00:00:00</td>\n",
       "      <td>2016-04-06 10:45:34</td>\n",
       "      <td>96224</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247867</th>\n",
       "      <td>bmw</td>\n",
       "      <td>3er</td>\n",
       "      <td>kombi</td>\n",
       "      <td>manuell</td>\n",
       "      <td>diesel</td>\n",
       "      <td>nein</td>\n",
       "      <td>3</td>\n",
       "      <td>150000</td>\n",
       "      <td>2005</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-04-02 00:00:00</td>\n",
       "      <td>2016-04-06 20:47:12</td>\n",
       "      <td>81825</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247868</th>\n",
       "      <td>seat</td>\n",
       "      <td>leon</td>\n",
       "      <td>limousine</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ja</td>\n",
       "      <td>225</td>\n",
       "      <td>150000</td>\n",
       "      <td>2004</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-03-19 00:00:00</td>\n",
       "      <td>2016-03-19 20:44:43</td>\n",
       "      <td>96465</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247869</th>\n",
       "      <td>smart</td>\n",
       "      <td>fortwo</td>\n",
       "      <td>cabrio</td>\n",
       "      <td>automatik</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>101</td>\n",
       "      <td>125000</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-03-05 00:00:00</td>\n",
       "      <td>2016-03-11 18:17:12</td>\n",
       "      <td>26135</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247870</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>transporter</td>\n",
       "      <td>bus</td>\n",
       "      <td>manuell</td>\n",
       "      <td>diesel</td>\n",
       "      <td>nein</td>\n",
       "      <td>102</td>\n",
       "      <td>150000</td>\n",
       "      <td>1996</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-03-19 00:00:00</td>\n",
       "      <td>2016-04-07 07:15:26</td>\n",
       "      <td>87439</td>\n",
       "      <td>9200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247871</th>\n",
       "      <td>bmw</td>\n",
       "      <td>m_reihe</td>\n",
       "      <td>limousine</td>\n",
       "      <td>manuell</td>\n",
       "      <td>benzin</td>\n",
       "      <td>nein</td>\n",
       "      <td>320</td>\n",
       "      <td>50000</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-03-07 00:00:00</td>\n",
       "      <td>2016-03-22 03:17:10</td>\n",
       "      <td>73326</td>\n",
       "      <td>28990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247872 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             brand        model vehicleType    gearbox fuelType  \\\n",
       "0       volkswagen         golf  kleinwagen    manuell   benzin   \n",
       "1            skoda        fabia  kleinwagen    manuell   diesel   \n",
       "2              bmw          3er   limousine    manuell   benzin   \n",
       "3          peugeot      2_reihe      cabrio    manuell   benzin   \n",
       "4            mazda      3_reihe   limousine    manuell   benzin   \n",
       "...            ...          ...         ...        ...      ...   \n",
       "247867         bmw          3er       kombi    manuell   diesel   \n",
       "247868        seat         leon   limousine    manuell   benzin   \n",
       "247869       smart       fortwo      cabrio  automatik   benzin   \n",
       "247870  volkswagen  transporter         bus    manuell   diesel   \n",
       "247871         bmw      m_reihe   limousine    manuell   benzin   \n",
       "\n",
       "       notRepairedDamage  powerPS  kilometer  yearOfRegistration  \\\n",
       "0                   nein       75     150000                2001   \n",
       "1                   nein       69      90000                2008   \n",
       "2                     ja      102     150000                1995   \n",
       "3                   nein      109     150000                2004   \n",
       "4                   nein      105     150000                2004   \n",
       "...                  ...      ...        ...                 ...   \n",
       "247867              nein        3     150000                2005   \n",
       "247868                ja      225     150000                2004   \n",
       "247869              nein      101     125000                2000   \n",
       "247870              nein      102     150000                1996   \n",
       "247871              nein      320      50000                2013   \n",
       "\n",
       "        monthOfRegistration          dateCreated             lastSeen  \\\n",
       "0                         6  2016-03-17 00:00:00  2016-03-17 17:40:17   \n",
       "1                         7  2016-03-31 00:00:00  2016-04-06 10:17:21   \n",
       "2                        10  2016-04-04 00:00:00  2016-04-06 19:17:07   \n",
       "3                         8  2016-04-01 00:00:00  2016-04-05 18:18:39   \n",
       "4                        12  2016-03-26 00:00:00  2016-04-06 10:45:34   \n",
       "...                     ...                  ...                  ...   \n",
       "247867                    5  2016-04-02 00:00:00  2016-04-06 20:47:12   \n",
       "247868                    5  2016-03-19 00:00:00  2016-03-19 20:44:43   \n",
       "247869                    3  2016-03-05 00:00:00  2016-03-11 18:17:12   \n",
       "247870                    3  2016-03-19 00:00:00  2016-04-07 07:15:26   \n",
       "247871                    8  2016-03-07 00:00:00  2016-03-22 03:17:10   \n",
       "\n",
       "        postalCode  price  \n",
       "0            91074   1500  \n",
       "1            60437   3600  \n",
       "2            33775    650  \n",
       "3            67112   2200  \n",
       "4            96224   2000  \n",
       "...            ...    ...  \n",
       "247867       81825   3999  \n",
       "247868       96465   3200  \n",
       "247869       26135   1199  \n",
       "247870       87439   9200  \n",
       "247871       73326  28990  \n",
       "\n",
       "[247872 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 247872 entries, 0 to 247871\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   brand                247872 non-null  object\n",
      " 1   model                247872 non-null  object\n",
      " 2   vehicleType          247872 non-null  object\n",
      " 3   gearbox              247872 non-null  object\n",
      " 4   fuelType             247872 non-null  object\n",
      " 5   notRepairedDamage    247872 non-null  object\n",
      " 6   powerPS              247872 non-null  int64 \n",
      " 7   kilometer            247872 non-null  int64 \n",
      " 8   yearOfRegistration   247872 non-null  int64 \n",
      " 9   monthOfRegistration  247872 non-null  int64 \n",
      " 10  dateCreated          247872 non-null  object\n",
      " 11  lastSeen             247872 non-null  object\n",
      " 12  postalCode           247872 non-null  int64 \n",
      " 13  price                247872 non-null  int64 \n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 26.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info() #  Пропусков нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand 39\n",
      "model 250\n",
      "vehicleType 8\n",
      "gearbox 2\n",
      "fuelType 7\n",
      "notRepairedDamage 2\n",
      "powerPS 496\n",
      "kilometer 13\n",
      "yearOfRegistration 85\n",
      "monthOfRegistration 13\n",
      "dateCreated 107\n",
      "lastSeen 129878\n",
      "postalCode 8070\n",
      "price 4951\n"
     ]
    }
   ],
   "source": [
    "for column in df:\n",
    "    print(column, len(df[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по колличеству уникальных значений можно почти все признаки сделать категориальными, но все-таки разумнее сделать пробег и мощность численной. Учитывая, что выборка взята за один и тот же год, дата создания нам ничего не скажет, а вот последний просмотр может показывать насколько адекватная цена. Посчитаем максимальное время просмотра и потом для каждой машины посчитаем насколько давно был последний просмотр в днях. Год и месяц регистрации дадут инфу про старость машины. Почтовый код может что-то сказать о ценах в регионе где продается машина, но мы его брать не будем потому что из-за него 8к столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lastSeen = pd.to_datetime(df.lastSeen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = df.lastSeen.apply(lambda row: row.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_day = max(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sinceLastSeen'] = max_day - day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notRepairedDamage = (df.notRepairedDamage == 'ja') + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.monthOfRegistration = df.yearOfRegistration * 12 + df.monthOfRegistration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categorical = ['brand', 'model', 'vehicleType', 'gearbox', 'fuelType', 'notRepairedDamage', ]\n",
    "numeric_features = ['sinceLastSeen', 'kilometer', 'powerPS', 'monthOfRegistration']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
    "    ('scaling', StandardScaler(), numeric_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['price', 'yearOfRegistration', 'dateCreated', 'lastSeen', 'postalCode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = column_transformer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index().price\n",
    "y_validation = y_validation.reset_index().price\n",
    "y_test = y_test.reset_index().price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49575, 312)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49575,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'utils.GradientDescent'>\n",
      "\n",
      "Loss 66.8873446377246 lambda 0.001\n",
      "Loss 58.623856788411835 lambda 0.0021544346900318843\n",
      "Loss 36.66689016262516 lambda 0.004641588833612777\n",
      "Loss 13.244727610792454 lambda 0.01\n",
      "Loss 3.7768395092088256 lambda 0.021544346900318832\n",
      "Loss 1.6992286703262007 lambda 0.046415888336127774\n",
      "Loss 1.14081391296681 lambda 0.1\n",
      "Loss 0.7160050501864235 lambda 0.21544346900318823\n",
      "Loss 0.5166859673459074 lambda 0.46415888336127775\n",
      "Loss 0.43933921015014377 lambda 1.0\n",
      "\n",
      "\n",
      "\n",
      "<class 'utils.StochasticDescent'>\n",
      "\n",
      "Loss 65.92910883550779 lambda 0.001\n",
      "Loss 54.60055623282848 lambda 0.0021544346900318843\n",
      "Loss 30.590550348084342 lambda 0.004641588833612777\n",
      "Loss 12.922438133869012 lambda 0.01\n",
      "Loss 4.457080804401183 lambda 0.021544346900318832\n",
      "Loss 4.9243765094565894 lambda 0.046415888336127774\n",
      "Loss 1.960019368651537 lambda 0.1\n",
      "Loss 2.1065981788565344 lambda 0.21544346900318823\n",
      "Loss 3.7436148359973065 lambda 0.46415888336127775\n",
      "Loss 369624.2703363052 lambda 1.0\n",
      "\n",
      "\n",
      "\n",
      "<class 'utils.MomentumDescent'>\n",
      "\n",
      "Loss 66.41573804715851 lambda 0.001\n",
      "Loss 56.39587082931604 lambda 0.0021544346900318843\n",
      "Loss 32.967748420382925 lambda 0.004641588833612777\n",
      "Loss 11.09605027163435 lambda 0.01\n",
      "Loss 3.2522771778704973 lambda 0.021544346900318832\n",
      "Loss 1.5925262865530407 lambda 0.046415888336127774\n",
      "Loss 1.0717132551792168 lambda 0.1\n",
      "Loss 0.6746592041919496 lambda 0.21544346900318823\n",
      "Loss 0.5015142733087108 lambda 0.46415888336127775\n",
      "Loss 0.43179744333226155 lambda 1.0\n",
      "\n",
      "\n",
      "\n",
      "<class 'utils.Adagrad'>\n",
      "\n",
      "Loss 69.03492191993467 lambda 0.001\n",
      "Loss 68.73280208439392 lambda 0.0021544346900318843\n",
      "Loss 67.85087109840029 lambda 0.004641588833612777\n",
      "Loss 65.66976195199308 lambda 0.01\n",
      "Loss 60.27582302803904 lambda 0.021544346900318832\n",
      "Loss 48.30467278670247 lambda 0.046415888336127774\n",
      "Loss 26.20957431165027 lambda 0.1\n",
      "Loss 3.881850323163883 lambda 0.21544346900318823\n",
      "Loss 0.41266698678393643 lambda 0.46415888336127775\n",
      "Loss 0.3551552738249573 lambda 1.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "w0 = np.zeros(X_train.shape[1])\n",
    "lambdas = np.logspace(-3, 0, 10)\n",
    "max_iter_by_class = {\n",
    "    Adagrad: 1000,\n",
    "    MomentumDescent: 1000,\n",
    "    GradientDescent: 1000,\n",
    "    StochasticDescent: 1000\n",
    "}\n",
    "for descent_class in [\n",
    "    GradientDescent,\n",
    "    StochasticDescent,\n",
    "    MomentumDescent,\n",
    "    Adagrad,\n",
    "]:\n",
    "    print(descent_class)\n",
    "    print()\n",
    "    for lambda_ in lambdas:\n",
    "        if descent_class == StochasticDescent:\n",
    "            descent = descent_class(lambda_=lambda_, w0=w0, batch_size=2)\n",
    "        else:\n",
    "            descent = descent_class(lambda_=lambda_, w0=w0)\n",
    "        regression = LinearRegression(descent=descent, tolerance=0.0001, max_iter=max_iter_by_class[descent_class])\n",
    "        regression.fit(X_validation, y_validation)\n",
    "        \n",
    "        print('Loss', regression.loss_history[-1], 'lambda', lambda_)\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'utils.GradientDescent'>\n",
      "\n",
      "Loss 0.43933921015014377 lambda 1.0\n",
      "Loss 0.43205841863352745 lambda 1.107756850509709\n",
      "Loss 0.4259909372477563 lambda 1.22712523985119\n",
      "Loss 0.4198140743381075 lambda 1.3593563908785256\n",
      "Loss 0.4141182282952954 lambda 1.5058363542798405\n",
      "Loss 0.4088735391591837 lambda 1.6681005372000588\n",
      "Loss 0.4044661399728187 lambda 1.847849797422291\n",
      "Loss 0.3983547920444579 lambda 2.046968271807521\n",
      "Loss 0.39184649340181 lambda 2.2675431258708016\n",
      "Loss 0.38648631613512313 lambda 2.51188643150958\n",
      "\n",
      "\n",
      "\n",
      "<class 'utils.MomentumDescent'>\n",
      "\n",
      "Loss 0.43179744333226155 lambda 1.0\n",
      "Loss 0.4257537966878058 lambda 1.107756850509709\n",
      "Loss 0.41959754806934335 lambda 1.22712523985119\n",
      "Loss 0.41391799795817963 lambda 1.3593563908785256\n",
      "Loss 0.4086873396158279 lambda 1.5058363542798405\n",
      "Loss 0.40429138177745805 lambda 1.6681005372000588\n",
      "Loss 0.39988397476045545 lambda 1.847849797422291\n",
      "Loss 0.39622287450331267 lambda 2.046968271807521\n",
      "Loss 0.39232598899594645 lambda 2.2675431258708016\n",
      "Loss 0.38719972549606624 lambda 2.51188643150958\n",
      "\n",
      "\n",
      "\n",
      "<class 'utils.Adagrad'>\n",
      "\n",
      "Loss 0.3551552738249573 lambda 1.0\n",
      "Loss 0.35402510364325834 lambda 1.107756850509709\n",
      "Loss 0.35319293624253595 lambda 1.22712523985119\n",
      "Loss 0.3526074261374927 lambda 1.3593563908785256\n",
      "Loss 0.3522007702766346 lambda 1.5058363542798405\n",
      "Loss 0.3518640657486462 lambda 1.6681005372000588\n",
      "Loss 0.3516084212941402 lambda 1.847849797422291\n",
      "Loss 0.351416474263184 lambda 2.046968271807521\n",
      "Loss 0.3513047889385404 lambda 2.2675431258708016\n",
      "Loss 0.35125856778211156 lambda 2.51188643150958\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "w0 = np.zeros(X_train.shape[1])\n",
    "lambdas = np.logspace(0, 0.4, 10)\n",
    "max_iter_by_class = {\n",
    "    Adagrad: 1000,\n",
    "    MomentumDescent: 1000,\n",
    "    GradientDescent: 1000,\n",
    "    StochasticDescent: 1000\n",
    "}\n",
    "for descent_class in [\n",
    "    GradientDescent,\n",
    "#     StochasticDescent,\n",
    "    MomentumDescent,\n",
    "    Adagrad,\n",
    "]:\n",
    "    print(descent_class)\n",
    "    print()\n",
    "    for lambda_ in lambdas:\n",
    "        if descent_class == StochasticDescent:\n",
    "            descent = descent_class(lambda_=lambda_, w0=w0, batch_size=2)\n",
    "        else:\n",
    "            descent = descent_class(lambda_=lambda_, w0=w0)\n",
    "        regression = LinearRegression(descent=descent, tolerance=0.0001, max_iter=max_iter_by_class[descent_class])\n",
    "        regression.fit(X_validation, y_validation)\n",
    "        print('Loss', regression.loss_history[-1], 'lambda', lambda_)\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 10)\n",
    "\n",
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7. Регуляризация (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние регуляризации на работу различных методов градиентного спуска. (Напомним, регуляризация - это добавка к функции потерь, которая штрафует за норму весов). Мы будем использовать l2 регуляризацию, таким образом функция потерь приобретает следующий вид:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2 + \\dfrac{1}{2} \\| w \\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допишите классы **GradientDescentReg**, **StochasticDescentReg**, **MomentumDescentReg**, **AdagradReg** в файле `utils.py`. Мы будем использовать тот же самый класс для линейной регрессии, так как для сравнения методов с регуляризацией и без неё нам нужна только MSE часть функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите по валидационной выборке лучшие параметры обучения с регуляризацией. Сравните для каждого метода результаты на тестовой выборке по метрикам MSE и R^2 с регуляризацией и без регуляризации. Постройте для каждого метода график со значениями функции потерь MSE с регуляризацией и без регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите на получившиеся результаты. Какие можно сделать выводы, как регуляризация влияет на сходимость? Чем вы можете объяснить это?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils import (\n",
    "    AdagradReg,\n",
    "    GradientDescentReg,\n",
    "    MomentumDescentReg,\n",
    "    StochasticDescentReg,\n",
    ")\n",
    "\n",
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8. Бонус — Реализация метода SAG (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве бонуса вам будет следующее задание - напишите собственную реализацию стохастического градиентного спуска по методу SAG в файле `utils.py`. Подробнее прочитать про SAG можно [здесь](https://arxiv.org/pdf/1309.2388.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните свою реализацию метода SAG с обычным полным градиентным спуском на наших данных. Проведите сравнение аналогично заданию 5. Что вы можете сказать про сходимость этого метода?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставьте картинку или видео, описывающие ваш опыт выполнения этого задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
